% \documentclass[a4paper,man,natbib]{apa6}
\documentclass[11pt]{article}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{listings}
\usepackage{float}
\usepackage[section]{placeins}
\setlength{\parindent}{0in}

% library(knitr)
%\VignetteIndexEntry{Partools}


\title{Rectools}
% \shorttitle{Your APA6-Style Manuscript}
\author{Pooja Rajkumar, Norman Matloff}
% \affiliation{University of California, Davis}

% \abstract{Recommendation engines have a number of different
% applications. From books to movies, they enable the analysis and
% prediction of consumer preferences. The prevalence of recommender
% systems in both the business and computational world has led to clear
% advances in prediction models over the past years. Current R packages
% include recosystem and recommenderlab. However, our new package,
% rectools, currently under development, extends its capabilities in
% several directions. One of the most important differences is that
% rectools allows users to incorporate covariates, such as age and gender,
% to improve predictive ability and better understand consumer behavior.
% Our software incorporates a number of different methods, such as
% non-negative matrix factorization, random effects models, and nearest
% neighbor methods. In addition to our incorporation of covariate
% capabilities, rectools also integrates several kinds of parallel
% computation.}

\begin{document}
\SweaveOpts{concordance=TRUE}

\maketitle   

{\Large UNDER CONSTRUCTION}

\section{Recommendation Engines} 

A recommender system is an engine to predict the rating or preference
that a user would give an item. E-commerce websites in particular rely heavily on recommender systems to improve customer satstifaction, predict consumer trends, and develop products based on consumer tastes.
%which traditionally utilize collaborative filtering for their methods. 
This package builds on previous methodology/packages by including several
interesting novel features, notably (a) inclusion of covariates and (b)
parallelized computation.

\section{Methods}

We have a data matrix of user ratings of some items.  For concreteness,
let's use the common example of users rating movies.  

Though a nearest-neighbor approach is planned for addition to {\bf
rectools}, at present the package is primarily focused on what are
variously called {\it latent factor} (Koren, {\it et al}, 2009), {\it
baseline predictors} (Ekstrand {\it et al}, 2010) or {\it bias term}
(Koren {\it et al}, 2009) approaches:

\subsection{Matrix Factorization Approach}

Let $A$ denote the full ratings matrix, with one row for each user and
one column for each item.  Thus the dimension of $A$ is $m \times n$,
where $m$ and $n$ are the numbers of users and items, respectively.

Most of $A$ consists of unknown quantities, but we wish to find known
matrices whose product approximates $A$:

\begin{equation}
A \approx P Q
\end{equation}

where the matrices $P$ and $Q$ have dimensions $m \times k$ and $k
\times n$, respectively.  The numbers of columns of $P$ and rows of $Q$,
$k$, are chosen to be much less than $m$ and $n$, to avoid overfitting.
In this manner, we formulate predictions for the unknown entries of $A$.

The intuition is as follows.  Consider the matrix $Q$.  It has a column
for each item, so a row corresponds loosely to a set of ratings of all
the items.  They are not true ratings, as they will be multiplied by
numbers in $P$, but again loosely speaking, we have found $k$ typical
user rating patterns that summarize user behavior.

Similarly, the $k$ columns of $P$ summarize item ``behavior,'' i.e.\ how
items vary from one another.

\subsection{ANOVA Models}

These take a statistical Analysis of Variance approach.  The rating by
user $i$ of item $j$, denoted $Y_{ij}$, is assumed to have the form

\begin{equation}
Y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}
\end{equation}

Here $\alpha_i$ is the tendency of user $i$ to rate items higher or
lower than does the average user; $\beta_j$ is the tendency of item
$j$ to be rated higher or lower than the average item (i.e.\ this item's
relative popularity); and $\epsilon_{ij}$ is the combined effect of all
unknown factors, e.g.\ the user's mood at the time the rating is made.
The terms $\alpha_i$, $\beta_j$ and $\epsilon_{ij}$ are modeled as
random variables, so we have a {\it random effects model} in ANOVA
terminlogy, with mean 0.  The parameter $\mu$ is a fixed but unknown
constant, equal to the theoretical average of all possible items by all
possible users.

To do prediction, one uses the known user-item data to form estimates of
$\mu$ and the $\alpha_i$ and $\beta_j$.  Using the statistical ``hat''
notation to denote ``estimate of,'' our predicted rating of item $j$
by user $i$ is

\begin{equation}
\label{anovapred}
\widehat{Y}_{ij} = \widehat{\mu} + \widehat{\alpha}_i +
\widehat{\beta}_j
\end{equation}

The issue then is a matter of deciding how to obtain the estimates on
the right-hand side of (\ref{anovapred}).

Random effects ANOVA models have been used since the early years of
statistics.  Typically the $\alpha_i$, $\beta_j$ and $\epsilon_{ij}$ are
assumed to have normal distributions (with different variances).  The
quantities in (\ref{anovapred}) are then obtained by the Maximum Likelihood
Estimation (MLE) approach.

The R package $lme4$ is widely used for random effects models, and is
one of the ingredients in {\bf rectools}.

An alternative is to use another time-honored statistical technique, the
Method of Moments (MM) (Owen and Gao, 2015; Perry 2015), as follows:

Define

\begin{equation}
\label{yidot}
Y_{i.} = \frac{1}{N_i} \sum_{j=1}^{N_i} Y_{ij}
\end{equation}

\begin{equation}
\label{ydotj}
Y_{.j} = \frac{1}{N_j} \sum_{i=1}^{N_i} Y_{ij}
\end{equation}

\begin{equation}
\label{ydotdot}
Y_{..} =
 \sum_{i=1}^{N_i} 
\sum_{j=1}^{N_j} Y_{ij}
\end{equation}

where $N_i$ is the number of items rated by user $i$, $N_j$ is the
number of users rating item $j$, and $N$ is the sum of all the $N_i$
(same as sum of all the $N_j$).

Due to the mean-0 nature of the various terms, (\ref{yidot}), (\ref{ydotj})
and (\ref{ydotdot}) have expected values 
$\alpha_i + \mu$,
$\beta_j + \mu$ and 
$\alpha_i + \beta_j + \mu$, respectively.  This leads to the natural
estimates

\begin{equation}
\widehat{\alpha}_i = Y_{i.} - Y_{..}
\end{equation}

\begin{equation}
\widehat{\beta}_j = Y_{.j} - Y_{..}
\end{equation}

\begin{equation}
\widehat{\mu}_j = Y_{..}
\end{equation}

These are used in (\ref{anovapred}) to obtain our actual predicted
ratigs.


\subsection{findYdots}

findYdots allows us to use a latent factor model to predict values in our data set. Ydots makes use of the following equation: 


Suppose we have a user Ali. Ali has seen the following movies: 


\begin{tabular}{l l l}

\textbf{userID} & \textbf{movieID} & \textbf{rating}\\

13 & 10& 2\\ 2 & 100 & 3 \\ .. & ... & ... \\ \end{tabular} \\ Our
estimated $\alpha_i$ here is the tendency for a user to rate a
particular item compared to everyone else. Thus, the $\alpha_{Ali}$ here
would be Ali's tendency to be an "easy" or "hard" rater. For example, if
the average rating for movies is a 3 and Ali is generally harsh grader
(with primarily 1s and 2s) then Ali's $\alpha_{Ali}$ would
be negative. Vice versa, Ali's alpha would be positive if Ali is a
generally easy grader. 

Our estimated $\beta_j$ is the tendency for the movie to
be rated really highly or poorly. For example, a movie that does really
well will have generally high ratings, and thus, a generally high
$\beta_j$. If a movie has generally poor ratings, then the
$\beta_j$ will be low.

\subsection{Method of Moments}

Method of Moments regresses the ratings against covariates such as age,
gender, or genre. We then subtract these predictions from the actual
value and apply the latent factor model. In order to use method of
moments, make sure your data set (called ratingsIn) follows the same
format as below: 


\begin{tabular}{l l l}

\textbf{userID} & \textbf{itemID} & \textbf{rating}\\

8 & 1& 2\\
99 & 5& 5 \\
.. & ... & ... \\
\end{tabular}
\\
\subsection{An Example}
Suppose we have a user named Karina. Suppose we have the following subsection of ratings: 
\\
\\
\begin{tabular}{l l l}

\textbf{userID} & \textbf{itemID} & \textbf{rating}\\

Karina & The Little Mermaid& 2\\
Karina & The Avengers& 5 \\
Karina & Captain America & 4\\
Karina & Iron Man & ???\\
John   & Iron Man& 4\\
Max    & Iron Man & 5 \\

\end{tabular}
\\
\\

From the looks of it, Karina seems to enjoy action/superhero movies. Given this data set, lets predict how Karina would enjoy the movie Iron Man. Thus, in this case, we want to find out our predicted: \begin{equation} Y_{Karina, Iron Man} \end{equation}
Thus, we plug into the equations above: 

\begin{equation}
\widehat{\alpha}_{Karina} = Y_{Karina.} - Y_{..}
\end{equation}

Here, $Y_{..}$ = $\frac{2 + 5 + 4 + 4 + 5}{5}$ = 4, which is the average rating for all movies by all users. 

Here, our $Y_{Karina}$ = $\frac{2 + 5 + 4}{3}$ = 3.22 - 4 = -0.78 The 2,5, and 4 refer to Karina's ratings for The Little Mermaid, The Avengers, and Captain America respectively. After subtracting, we find that Katrina's estimated alpha is negative, thus implying that Katrina is a rather harsh grader.


\begin{equation}
\widehat{\beta}_{Iron Man} = Y_{.Iron Man} - Y_{..}
\end{equation}

Next, our $\widehat{\beta}_{Iron Man}$  =  $\frac{4 + 5}{2}$ = 4.5 - 4 = 0.5. The 4 and 5 refer to all the ratings for Iron man, which were given be John and Max respectively. After subtracting from overall ratings, we discover that Iron Man has a positive expected beta value. Thus, Iron Man generally received high reviews and was well liked in this data set.


\begin{equation}
\widehat{\mu}_{Iron Man} = Y_{..}
\end{equation}

Lastly, $\widehat{\mu}_{Iron Man}$ is simply 4.

In order to get our predicted value: 
\begin{equation}
\widehat{Y}_{karina, Iron Man} = \widehat{\mu} + \widehat{\alpha}_{Karina} + \widehat{\beta}_{Iron Man}
\end{equation}


Thus, $\widehat{Y}_{karina, Iron Man}$ = 4 + (-0.78) + 0.5 = 3.72. Our predicted value for Katrina's rating for Iron Man is thus 3.72. 

\subsection{Movie Lens data (no covariates)}
Now, we will walk through a step by step example. Suppose we have a movie lens data set, which can be found here: xyz. The three columns are referred to the userID, movieID, and rating for that particular user and movie on a scale of 1-5 respectively. For the sake of convenience, we will rename the three columns like so: 

%NM ADD CODE HERE 
%NM 
%NM Suppose we take a snippet of the following data set: 
%NM 
%NM \begin{tabular}{l l l}
%NM 
%NM \textbf{userID} & \textbf{movieID} & \textbf{rating}\\
%NM 
%NM 196 & 242 & 3\\ 186 & 302 & 3 \\ 22 & 377 & 1 \\
%NM 244 & 51 & 2 \\ 166 & 346 & 1 \\ 298 & 474 & 4 \\
%NM 115 & 265 & 2 \\ 253 & 465 & 5 \\ 305 & 451 & 3 \\
%NM .. & .. & .. \\
%NM 
%NM \\ \end{tabular}
%NM 
%NM Suppose we want to predict the rating user 196 would give movie 2. We would first make a call to findYdots. There are two seperate ways that we can predict- using either findYdotsMM or findYdotsMLE. The first uses method of moments while the later uses maximum likeihood methods. 
%NM \\ Supose we set the movie lens data set under an alias, data. Looking at findYdotsMM first, we make the following call: 
%NM \\
%NM dataMeans = findYdotsMM(data) 
%NM \\
%NM This call returns the overall mean of the entire data set, the mean of all users, and the mean of all items, like so: 
%NM \\
%NM ADD PICTURE HERE 
%NM \\
%NM These means can be accessed quite easily: 
%NM overall_mean = dataMeans$grandMeans
%NM user_mean = dataMeans$usrMeans
%NM item_mean = dataMeans$itmMeans
%NM \\
%NM Note: itmMeans and usrMeans are both vectors, because they are the means of each individual user or movie. \\
%NM Additionally, findYdotsMM has the option to simply regress the estimated mean coefficents, by signaling regressYdots like so: 
%NM \\
%NM test = findYdotsMM(data, regressYdots = TRUE)
%NM \\
%NM This call returns all the means as well as the regressYdots attribute, which is the coefficents of the regressed means, which can be found like so:\\
%NM coef_means - test$regressYdots \\
%NM Which looks like so: \\
%NM 
%NM ADD PICTURE HERE 
%NM 
%NM \\
%NM Additionally, findYdots has a parallelization feature for computational speed up and large data sets. \\
%NM vals = findYdots(data, regressYdots = FALSE, cls)\\
%NM \\
%NM A helpful feature for users who wish to use one data set to predict another, there is a predict function for ydotsMM. Predict requires a ydotsObj and a testSet. Suppose in this case, we want to predict on the movielens data set, from our own data set: 
%NM \\
%NM testSet = data
%NM testSet$rating = 0
%NM val = predict(dataMeans,testSet)
%NM \\
%NM val here would simply be an array of predicted values for each user and each movie in the movielens data set, like so: 
%NM \\
%NM ENTER A PICTURE HERE 
%NM \\
%NM Suppose we wanted to predict using maximum likeihood methods:\\
%NM res_mle = findYdotsMLE(data) \\
%NM The means are fitted to the lme4 model and returned as a ydotsMLE type: 
%NM \\ ENTER A PICTURE HERE \\
%NM Parallelization is also a feature for lme4: \\
%NM res_mle = findYdotsMLE(data, cls) \\
%NM Predict is also enabled for findYdotsMLE. \\
%NM pred_val = predict(res_mle, testSet)\\
%NM pred_val contains a 
%NM  
%NM \subsection{How findYdots is called}
%NM 
%NM \begin{lstlisting}
%NM findYdotsMM <- function(ratingsIn,regressYdots=FALSE,cls=NULL)
%NM findYdotsMLE <- function(ratingsIn,cls=NULL)
%NM 
%NM \end{lstlisting}
%NM Both functions will return different classes. 
%NM \\
%NM findYdotsMM will return a class called ydotsMM, which contains a list of the overall mean ($Y_..$), the user means ($Y_i.$) the item means ($Y_.j$) and, if enabled, the regressed means (refer to regressYdots below). 
%NM \\
%NM findYdotsMLE will return a class called ydotsMLEpar, which contains $Y_..$, $Y_i.$, $Y_.j$. 
%NM 
%NM \subsection{regressYdots}
%NM 
%NM If regressYdots is true, apply lm to the estimated latent factors and
%NM their product, enabling rating prediction from the resulting linear
%NM function of the factors. This is currently implemented as if there are
%NM no covariates.  
%NM 
%NM \subsection{Prediction} 
%NM Additionally, findYdots has a predict extension on R. Thus, you can call predict.ydotsMM to call method of moments on your data set like so:
%NM predict.ydotsMM(ydotsObj,testSet), where ydotsObj is an object of type findYdots, and testSet is in the same form of ratingsIn. Similiarly, you can call predict.ydotsMLE(ydotsObj,testSet) in the same fashion. Both will return a set of predicted values.
%NM 
%NM \subsection{cls} 
%NM 
%NM Our package is parallelized in all capacities. Thus, if a user would like to improve their computation time, a user can send in nodes to the function itself, and findydots will parallelize as needed.
%NM 
%NM \subsection{xval}
%NM Additionally, you can use xvalMLE or xvalMM to split your input data into training and test sets and then determine the accuracy of your predicted values by training part of your data set to predict known values in your test set. 
%NM 
%NM \subsection{xvalMLE}
%NM xvalMLE can be called: 
%NM \\
%NM xvalMLE(ratingsIn, trainprop = 0.5, 'exact')
%NM \\
%NM When using xvalMLE, you will need a data set in the same form of ratingsIn (as discussed above), with the option of adding a training proportion, an accuracy measure, and nodes to parallelize. The training proportion will be the proportion of the data set you would like to train in order to predict your test set. Your accuracy measure can be one of three different possibilties: 
%NM \\
%NM 1. exact- The accuracy is measured based on the average of all the correct predicted values compared to the test set. \\
%NM 2. mad- The accuracy is measured based on the average of the absolute value of differences between the predicted value and actual value. \\
%NM 3. rms - The accuracy is measured by first subtracting the predicted and actual values, squaring it, taking the mean, and then taking the square root of the mean.\\
%NM \\
%NM xvalMLE returns a class called result of type xvalb, which contains the number of rows in the data set, the training proportion, the accuracy measure, the number of predicted NA values, and the actual calculated accuracy. 
%NM 
%NM \subsection{xvalMM}
%NM xValMM is similiar to xvalMLE, except it adds a regressYdots option as discussed above. It returns a class called result of type xvalb, which contains the number of rows in your data set, the training proportion, the accuracy measure, the number of predicted NA values, and the resulting accuracy. 
%NM 
%NM \subsection{xvalReco}
%NM xvalReco is an option for users who want to use a parallelized version of recosystem. xvalReco requires ratingsIn, but has the option to add training proportions, accuracy measures, clusters, and a rank. xvalReco not only calls recosystem on the data set, but also seperates the data set into training and test sets and calculated the accuracy of the cross validation. xvalReco also extends the predict capability, given a recoObj and a testset. A recoObj would be the object returned from recosystem. Thus, xvalReco returns the number of rows in ratingsIn, the training proportion, the accuracy measure, the number of predicted NA values, and the accuracy.xvalReco has additional helpful functions, such as: 
%NM \\
%NM 1. getTrainSet- returns a training set given a data set and an option to add a training proportion \\
%NM \\ 2. getTestSet- returns a testset given a data set and the training set
%NM \\
%NM 3. trainReco- runs recosystem on a training set, with additional options to add a training proportion and a rank, and returns a class res of type Reco which contains all the predicted user and movie values. \\
%NM \\
%NM 4. predict- the object returned by recosystem contains two vectors, P and Q, which contain the predicted values for each user and movie respectively. Thus, in order to get the actual predicted value for a particular user and movie, predict multiplies the p and q matricies. Once these are multiplied, we will have the correct predicted values for each $Y_ij$. 
%NM \\
%NM xvalReco calls trainset, then testset, then train, and then predict. Once the predicted values are returned, xvalReco calculates the accuracy. 
\section{References}

(To be filled in.)

Koren, 2009.  Art Owen and Kaitlyn Gao, 2015.  Patrick Perry, 2015.


\end{document}

